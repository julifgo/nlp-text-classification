{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T16:40:42.047784Z",
     "start_time": "2020-05-27T16:40:28.057194Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, 'code')\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "from data import  loadFile,removeValues,convertColumnToCategorical,categoryProportion,retrieveValues,categoryCount\n",
    "import functions as utils\n",
    "from featEng import (lemmatizePipeStep,htmlCleanerPipeStep,lowerCasesPipeStep,stopWordsPipeStep,stemmizePipeStep,\n",
    "word2vecPipeStep,tfIdfVectorizerPipeStep)\n",
    "from resampler import upsampleRandom,downsampleRandom,upsampleSvmSmote\n",
    "from reporter import giveScore,giveWordCloud,writePathologicalCases\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T16:40:49.124867Z",
     "start_time": "2020-05-27T16:40:42.050777Z"
    }
   },
   "outputs": [],
   "source": [
    "df = loadFile('../../data/it_tickets.xlsx') #THIS IS TOY DATA FOR CONFIDENTIALITY MATTERS\n",
    "df = removeValues(df,'State','') #Remove emtpy values\n",
    "df = removeValues(df,'State','Removed') #Remove 'Removed' State\n",
    "df = removeValues(df,'Component','')\n",
    "df = removeValues(df,'Type','')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize Proportion a little bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T16:40:49.619543Z",
     "start_time": "2020-05-27T16:40:49.130851Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Type'].value_counts().plot(kind='bar')\n",
    "categoryCount(df,'Type'),categoryCount(df,'Component')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T16:40:50.210962Z",
     "start_time": "2020-05-27T16:40:49.622535Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df['Component'].value_counts().plot(kind='bar')\n",
    "categoryProportion(df,'Type'),categoryProportion(df,'Component')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T18:28:17.312151Z",
     "start_time": "2020-04-17T18:28:17.232364Z"
    }
   },
   "source": [
    "# Type Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T16:43:52.409670Z",
     "start_time": "2020-05-27T16:40:50.214952Z"
    }
   },
   "outputs": [],
   "source": [
    "df_type = df.copy()\n",
    "df_type = convertColumnToCategorical(df_type,'Type')\n",
    "print(categoryProportion(df_type,'Type'))\n",
    "X = df_type.loc[:,['Title', \"Description\",'Type']]\n",
    "y = df_type.loc[:,['Type']]\n",
    "print(\"---Splitting dataset\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.33, \n",
    "#     random_state=42, \n",
    "    stratify=y \n",
    ")\n",
    "print(categoryCount(X_train,'Type'))\n",
    "\n",
    "print(\"---Random Upsampling\")\n",
    "X_train = upsampleRandom(X_train,'Type','Incident', 2) #duplicamos los datos del set de entrenamiento\n",
    "\n",
    "print(\"---Random Downsampling\")\n",
    "X_train = downsampleRandom(X_train,'Type','Requirement', 0.80) #Nos quedamos con el 80% de los datos\n",
    "\n",
    "y_train = X_train.loc[:,['Type']]\n",
    "print(categoryCount(X_train,'Type'))\n",
    "\n",
    "#Si nos equivocamos, cortamos aca.\n",
    "assert X_train.shape[0] == y_train.shape[0] , 'Must have same numbers of both training and target examples'\n",
    "\n",
    "type_pipeline_preprocess = Pipeline([\n",
    "    ('clean', htmlCleanerPipeStep()), \n",
    "    ('lower', lowerCasesPipeStep()),\n",
    "    ('stopwords', stopWordsPipeStep('../../data/other/stopwords.txt')),\n",
    "    ('lemmatize', lemmatizePipeStep())\n",
    "])\n",
    "\n",
    "type_pipeline_fe = Pipeline([\n",
    "    ('preprocess', type_pipeline_preprocess),\n",
    "    ('word2vec', word2vecPipeStep())\n",
    "])\n",
    "\n",
    "print(\"---Applying Feat Eng\")\n",
    "type_pipeline_fe.fit(X_train,y_train)\n",
    "X_train_transform = type_pipeline_fe.transform(X_train)\n",
    "y_train_transform = y_train\n",
    "print(X_train.shape,y_train.shape)\n",
    "\n",
    "# print(\"---Over sampling X_train\") NOT A GOOD IDEA FOR NLP. TRY IT IF YOU WANT\n",
    "# X_train_transform, y_train_transform=upsampleSvmSmote({'sampling_strategy':'minority',\n",
    "#                                                       'n_jobs':-1,\n",
    "#                                                       'k_neighbors':15,\n",
    "#                                                       'm_neighbors':5},\n",
    "#                                                       X_train_transform,y_train)\n",
    "le = preprocessing.LabelEncoder()\n",
    "y_train_transform = le.fit_transform(y_train_transform)\n",
    "print(X_train_transform.shape,y_train_transform.shape)\n",
    "print(\"Transforming test set\")\n",
    "X_test_transform=type_pipeline_fe.transform(X_test)\n",
    "y_test_transform = le.transform(y_test)\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T16:44:09.977948Z",
     "start_time": "2020-05-27T16:43:52.412625Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "# ,100,25,10\n",
    "clf_t_nn = MLPClassifier(\n",
    "    hidden_layer_sizes=(100,75),\n",
    "    activation='relu',\n",
    "    random_state=42,\n",
    "    tol=0.001,\n",
    "    alpha=1.3,\n",
    "    early_stopping=True,\n",
    "    n_iter_no_change=20,\n",
    "    validation_fraction=0.1,\n",
    "    verbose=False,\n",
    "    warm_start=False\n",
    ")\n",
    "\n",
    "\n",
    "_ = clf_t_nn.fit(X_train_transform,y_train_transform)\n",
    "\n",
    "labels = le.inverse_transform(np.unique(y_test_transform))\n",
    "predictions = clf_t_nn.predict(X_test_transform)\n",
    "predictions_proba = clf_t_nn.predict_proba(X_test_transform)\n",
    "giveScore(clf_t_nn,X_train_transform,y_train_transform,X_test_transform,y_test_transform,True,labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicciones y casos patologicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T16:44:10.538487Z",
     "start_time": "2020-05-27T16:44:09.979943Z"
    }
   },
   "outputs": [],
   "source": [
    "max_pred = np.amax(predictions_proba,axis=1)\n",
    "\n",
    "predicted_values = le.inverse_transform(np.array(predictions))\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_test_predicted = X_test.copy()\n",
    "X_test_predicted['Predicted'] = predicted_values\n",
    "X_test_predicted['Confidence'] = max_pred\n",
    "\n",
    "casos_malos = X_test_predicted[pd.Series(X_test_predicted['Type'] != X_test_predicted['Predicted'])]\n",
    "confianza_de_casos_malos = casos_malos['Confidence']\n",
    "#Estos son aquellos casos donde la prediccion fue con mucha confianza en el sentido inverso.\n",
    "casos_peores = casos_malos[pd.Series(casos_malos['Confidence'] > 0.9)]\n",
    "\n",
    "\n",
    "casos_buenos = X_test_predicted[pd.Series(X_test_predicted['Type'] == X_test_predicted['Predicted'])]\n",
    "confianza_de_casos_buenos = casos_buenos['Confidence']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.boxplot([confianza_de_casos_malos,confianza_de_casos_buenos],labels=['Erradas', 'Acertadas'])\n",
    "plt.title(\"Distribucion de Predicciones de Type\")\n",
    "plt.show()\n",
    "print(\"Valores de Distribucion de predicciones erradas\")\n",
    "#Descripcion general de los casos en donde predijimos mal\n",
    "print(confianza_de_casos_malos.describe())\n",
    "print('----------------------------------------------------')\n",
    "print(\"Valores de Distribucion de predicciones acertadas\")\n",
    "#Descripcion general de los casos en donde predijimos bien\n",
    "print(confianza_de_casos_buenos.describe())\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.title(\"Wrong Predictions Distribution\")\n",
    "plt.hist(confianza_de_casos_malos,10)\n",
    "plt.xlabel(\"predict probability\")\n",
    "plt.ylabel(\"# cases\")\n",
    "mean = np.mean(confianza_de_casos_malos)\n",
    "plt.axvline(mean, color='k', linestyle='dashed', linewidth=1)\n",
    "min_ylim, max_ylim = plt.ylim()\n",
    "plt.text(mean*1.05, max_ylim*0.9, 'Mean: {0:.3f}'.format(mean))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T16:44:49.564732Z",
     "start_time": "2020-05-27T16:44:10.544437Z"
    }
   },
   "outputs": [],
   "source": [
    "X_incident = retrieveValues(X_train,'Type','Incident')\n",
    "X_requirement = retrieveValues(X_train,'Type','Requirement')\n",
    "giveWordCloud(type_pipeline_preprocess.transform(X_incident),utils.get_stopwords()),giveWordCloud(type_pipeline_preprocess.transform(X_requirement),utils.get_stopwords())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T18:56:50.632074Z",
     "start_time": "2020-04-17T18:56:50.628088Z"
    }
   },
   "source": [
    "# Component Training\n",
    "\n",
    "###### Se debe volver a entrenar antes de cualquier cosa, pues los nombres de las variables son los mismos que en Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T16:45:32.836025Z",
     "start_time": "2020-05-27T16:44:49.565699Z"
    }
   },
   "outputs": [],
   "source": [
    "df_component = df.copy()\n",
    "df_component = removeValues(df_component,'Component','Help Desk')\n",
    "df_component = convertColumnToCategorical(df_component,'Component')\n",
    "print(categoryProportion(df_component,'Component'))\n",
    "X = df_component.loc[:,['Title', \"Description\",'Component']]\n",
    "y = df_component.loc[:,['Component']]\n",
    "print(\"---Splitting dataset\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.33, \n",
    "    random_state=42, \n",
    "    stratify=y \n",
    ")\n",
    "print(categoryProportion(X_train,'Component'))\n",
    "\n",
    "print(\"---Random Upsampling\")\n",
    "X_train = upsampleRandom(X_train,'Component','Recambio de PC', 3)#triplicamos los datos del set de entrenamiento\n",
    "X_train = upsampleRandom(X_train,'Component','Alta y Baja de Usuarios', 2) #triplicamos los datos del set de entrenamiento\n",
    "X_train = upsampleRandom(X_train,'Component','No Conformidad Compra / Garantia', 2)#duplicamos los datos del set de entrenamiento\n",
    "\n",
    "print(\"---Random Downsampling\")\n",
    "X_train = downsampleRandom(X_train,'Component','Administracion de Servidores y Herramientas', 0.75)\n",
    "\n",
    "\n",
    "y_train = X_train.loc[:,['Component']]\n",
    "print(categoryProportion(X_train,'Component'))\n",
    "\n",
    "#Si nos equivocamos, cortamos aca.\n",
    "assert X_train.shape[0] == y_train.shape[0] , 'Must have same numbers of both training and target examples'\n",
    "\n",
    "component_pipeline_preprocess = Pipeline([\n",
    "    ('clean', htmlCleanerPipeStep()), \n",
    "    ('lower', lowerCasesPipeStep()),\n",
    "    ('stopwords', stopWordsPipeStep('../../data/other/stopwords.txt')),\n",
    "    ('stemmize', stemmizePipeStep())\n",
    "])\n",
    "component_pipeline_fe = Pipeline([\n",
    "    ('preprocess', component_pipeline_preprocess),\n",
    "    ('tfidf', tfIdfVectorizerPipeStep({\n",
    "        'stop_words':utils.get_stopwords(), \n",
    "        'strip_accents':'unicode',\n",
    "        'use_idf':True,\n",
    "        'ngram_range':(1,3)\n",
    "    }))\n",
    "])\n",
    "\n",
    "print(\"---Applying Feat Eng\")\n",
    "component_pipeline_fe.fit(X_train,y_train)\n",
    "X_train_transform = component_pipeline_fe.transform(X_train)\n",
    "print(X_train.shape,y_train.shape)\n",
    "le = preprocessing.LabelEncoder()\n",
    "y_train_transform = le.fit_transform(y_train)\n",
    "\n",
    "print(\"Transforming test set\")\n",
    "X_test_transform=component_pipeline_fe.transform(X_test)\n",
    "y_test_transform = le.transform(y_test)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T16:45:34.149513Z",
     "start_time": "2020-05-27T16:45:32.840015Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "\n",
    "clf_c = SGDClassifier()\n",
    "clf_c.set_params(alpha=0.0005,learning_rate='optimal',penalty='l2',random_state=42,tol=0.0005,loss='modified_huber')\n",
    "\n",
    "training = clf_c.fit(X_train_transform,y_train)\n",
    "labels = le.inverse_transform(np.unique(y_test_transform))\n",
    "predictions = clf_c.predict(X_test_transform)\n",
    "predictions_proba = clf_c.predict_proba(X_test_transform)\n",
    "giveScore(clf_c,X_train_transform,y_train,X_test_transform,y_test,True,labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicciones y Casos Patologicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T16:45:34.940399Z",
     "start_time": "2020-05-27T16:45:34.151509Z"
    }
   },
   "outputs": [],
   "source": [
    "max_pred = np.amax(predictions_proba,axis=1)\n",
    "\n",
    "predicted_values = np.array(predictions)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_test_predicted = X_test.copy()\n",
    "X_test_predicted['Predicted'] = predicted_values\n",
    "X_test_predicted['Confidence'] = max_pred\n",
    "\n",
    "X_test_predicted\n",
    "casos_malos = X_test_predicted[pd.Series(X_test_predicted['Component'] != X_test_predicted['Predicted'])]\n",
    "confianza_de_casos_malos = casos_malos['Confidence']\n",
    "#Estos son aquellos casos donde la prediccion fue con mucha confianza en el sentido inverso.\n",
    "casos_peores = casos_malos[pd.Series(casos_malos['Confidence'] > 0.9)]\n",
    "\n",
    "# casos_peores\n",
    "casos_buenos = X_test_predicted[pd.Series(X_test_predicted['Component'] == X_test_predicted['Predicted'])]\n",
    "confianza_de_casos_buenos = casos_buenos['Confidence']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.boxplot([confianza_de_casos_malos,confianza_de_casos_buenos],labels=['Erradas', 'Acertadas'])\n",
    "plt.title(\"Distribucion de Predicciones de Component\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.title(\"Wrong Predictions Distribution\")\n",
    "plt.hist(confianza_de_casos_malos,10)\n",
    "plt.xlabel(\"predict probability\")\n",
    "plt.ylabel(\"# cases\")\n",
    "mean = np.mean(confianza_de_casos_malos)\n",
    "plt.axvline(mean, color='k', linestyle='dashed', linewidth=1)\n",
    "min_ylim, max_ylim = plt.ylim()\n",
    "plt.text(mean*1.05, max_ylim*0.9, 'Mean: {0:.3f}'.format(mean))\n",
    "plt.show()\n",
    "print(\"Valores de Distribucion de predicciones erradas\")\n",
    "#Descripcion general de los casos en donde predijimos mal\n",
    "print(confianza_de_casos_malos.describe())\n",
    "print('----------------------------------------------------')\n",
    "print(\"Valores de Distribucion de predicciones acertadas\")\n",
    "#Descripcion general de los casos en donde predijimos bien\n",
    "print(confianza_de_casos_buenos.describe())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "###### Se debe correr primero la celda que procesa para que X_train este bien definido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T16:46:33.430040Z",
     "start_time": "2020-05-27T16:45:34.943390Z"
    }
   },
   "outputs": [],
   "source": [
    "X_alta_baja_usuarios = retrieveValues(X_train,'Component','Alta y Baja de Usuarios')\n",
    "print(X_alta_baja_usuarios.shape)\n",
    "X_servidores = retrieveValues(X_train,'Component','Administracion de Servidores y Herramientas')\n",
    "print(X_servidores.shape)\n",
    "giveWordCloud(component_pipeline_preprocess.transform(X_alta_baja_usuarios),utils.get_stopwords()),giveWordCloud(component_pipeline_preprocess.transform(X_servidores),utils.get_stopwords())"
   ]
  }
 ],
 "metadata": {
  "gist": {
   "data": {
    "description": "nlp-tickets-tagger/notebooks/clean/Model.ipynb",
    "public": true
   },
   "id": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
